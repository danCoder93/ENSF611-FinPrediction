{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e0f9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pytorch-lightning 2.6.0\n",
      "Uninstalling pytorch-lightning-2.6.0:\n",
      "  Successfully uninstalled pytorch-lightning-2.6.0\n",
      "Requirement already satisfied: lightning==2.6.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: pytorch-forecasting==1.5.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: torchmetrics==1.8.2 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>5.4 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning==2.6.0) (6.0.3)\n",
      "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (2025.10.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning==2.6.0) (0.15.2)\n",
      "Requirement already satisfied: packaging<27.0,>=20.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning==2.6.0) (25.0)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning==2.6.0) (2.9.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning==2.6.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning==2.6.0) (4.15.0)\n",
      "Collecting pytorch-lightning (from lightning==2.6.0)\n",
      "  Using cached pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from torchmetrics==1.8.2) (2.3.3)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from pytorch-forecasting==1.5.0) (1.16.2)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from pytorch-forecasting==1.5.0) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from pytorch-forecasting==1.5.0) (1.7.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (3.13.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from lightning-utilities<2.0,>=0.10.0->lightning==2.6.0) (78.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting==1.5.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting==1.5.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting==1.5.0) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting==1.5.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting==1.5.0) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from torch<4.0,>=2.1.0->lightning==2.6.0) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from torch<4.0,>=2.1.0->lightning==2.6.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from torch<4.0,>=2.1.0->lightning==2.6.0) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from torch<4.0,>=2.1.0->lightning==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from tqdm<6.0,>=4.57.0->lightning==2.6.0) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning==2.6.0) (3.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting==1.5.0) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\john2\\anaconda3\\envs\\611\\lib\\site-packages (from jinja2->torch<4.0,>=2.1.0->lightning==2.6.0) (3.0.3)\n",
      "Using cached pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
      "Installing collected packages: pytorch-lightning\n",
      "Successfully installed pytorch-lightning-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pytorch-lightning\n",
    "!pip install lightning==2.6.0 pytorch-forecasting==1.5.0 torchmetrics==1.8.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "752b7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, Baseline\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7961778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>macd_line</th>\n",
       "      <th>macd_diff</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>rsi</th>\n",
       "      <th>vix_prct_returns</th>\n",
       "      <th>daily_prct_change</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>ev_ebitda</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>log_return</th>\n",
       "      <th>target</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>24.96</td>\n",
       "      <td>113856948</td>\n",
       "      <td>0.584986</td>\n",
       "      <td>0.073510</td>\n",
       "      <td>0.511476</td>\n",
       "      <td>73.167919</td>\n",
       "      <td>-6.093190</td>\n",
       "      <td>0.930044</td>\n",
       "      <td>16464.0</td>\n",
       "      <td>38.940719</td>\n",
       "      <td>-0.060932</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>25.21</td>\n",
       "      <td>157112212</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.532876</td>\n",
       "      <td>75.319795</td>\n",
       "      <td>7.786260</td>\n",
       "      <td>1.001603</td>\n",
       "      <td>16464.0</td>\n",
       "      <td>38.940719</td>\n",
       "      <td>0.077863</td>\n",
       "      <td>0.009966</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>3</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>24.92</td>\n",
       "      <td>111266284</td>\n",
       "      <td>0.614530</td>\n",
       "      <td>0.065323</td>\n",
       "      <td>0.549207</td>\n",
       "      <td>68.460985</td>\n",
       "      <td>9.206799</td>\n",
       "      <td>-1.150337</td>\n",
       "      <td>16464.0</td>\n",
       "      <td>38.940719</td>\n",
       "      <td>0.092068</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>4</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>25.18</td>\n",
       "      <td>111477272</td>\n",
       "      <td>0.625177</td>\n",
       "      <td>0.060776</td>\n",
       "      <td>0.564401</td>\n",
       "      <td>71.009868</td>\n",
       "      <td>-8.625162</td>\n",
       "      <td>1.043339</td>\n",
       "      <td>16464.0</td>\n",
       "      <td>38.940719</td>\n",
       "      <td>-0.086252</td>\n",
       "      <td>0.010379</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>5</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>24.63</td>\n",
       "      <td>132677620</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>0.568025</td>\n",
       "      <td>59.969013</td>\n",
       "      <td>14.691270</td>\n",
       "      <td>-2.184273</td>\n",
       "      <td>16464.0</td>\n",
       "      <td>38.940719</td>\n",
       "      <td>0.146913</td>\n",
       "      <td>-0.022085</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>6</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  close     volume  macd_line  macd_diff  macd_signal        rsi  \\\n",
       "0 2016-04-01  24.96  113856948   0.584986   0.073510     0.511476  73.167919   \n",
       "1 2016-04-04  25.21  157112212   0.618475   0.085599     0.532876  75.319795   \n",
       "2 2016-04-05  24.92  111266284   0.614530   0.065323     0.549207  68.460985   \n",
       "3 2016-04-06  25.18  111477272   0.625177   0.060776     0.564401  71.009868   \n",
       "4 2016-04-07  24.63  132677620   0.582520   0.014495     0.568025  59.969013   \n",
       "\n",
       "   vix_prct_returns  daily_prct_change   ebitda  ev_ebitda  daily_return  \\\n",
       "0         -6.093190           0.930044  16464.0  38.940719     -0.060932   \n",
       "1          7.786260           1.001603  16464.0  38.940719      0.077863   \n",
       "2          9.206799          -1.150337  16464.0  38.940719      0.092068   \n",
       "3         -8.625162           1.043339  16464.0  38.940719     -0.086252   \n",
       "4         14.691270          -2.184273  16464.0  38.940719      0.146913   \n",
       "\n",
       "   log_return    target  time_idx symbol  \n",
       "0    0.009257  0.000812         0   AAPL  \n",
       "1    0.009966  0.003644         3   AAPL  \n",
       "2   -0.011570  0.012852         4   AAPL  \n",
       "3    0.010379  0.014263         5   AAPL  \n",
       "4   -0.022085  0.000786         6   AAPL  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_tft_dataset.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Make sure dtype is correct\n",
    "df[\"time_idx\"] = df[\"time_idx\"].astype(\"int64\")\n",
    "df[\"symbol\"] = df[\"symbol\"].astype(\"category\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c18f834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e3815ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  close     volume  macd_line  macd_diff  macd_signal        rsi  \\\n",
      "0 2016-04-01  24.96  113856948   0.584986   0.073510     0.511476  73.167919   \n",
      "1 2016-04-04  25.21  157112212   0.618475   0.085599     0.532876  75.319795   \n",
      "2 2016-04-05  24.92  111266284   0.614530   0.065323     0.549207  68.460985   \n",
      "3 2016-04-06  25.18  111477272   0.625177   0.060776     0.564401  71.009868   \n",
      "4 2016-04-07  24.63  132677620   0.582520   0.014495     0.568025  59.969013   \n",
      "\n",
      "   vix_prct_returns  daily_prct_change   ebitda  ev_ebitda  daily_return  \\\n",
      "0         -6.093190           0.930044  16464.0  38.940719     -0.060932   \n",
      "1          7.786260           1.001603  16464.0  38.940719      0.077863   \n",
      "2          9.206799          -1.150337  16464.0  38.940719      0.092068   \n",
      "3         -8.625162           1.043339  16464.0  38.940719     -0.086252   \n",
      "4         14.691270          -2.184273  16464.0  38.940719      0.146913   \n",
      "\n",
      "   log_return    target  time_idx symbol  \n",
      "0    0.009257  0.000812         0   AAPL  \n",
      "1    0.009966  0.003644         3   AAPL  \n",
      "2   -0.011570  0.012852         4   AAPL  \n",
      "3    0.010379  0.014263         5   AAPL  \n",
      "4   -0.022085  0.000786         6   AAPL  \n",
      "Index(['date', 'close', 'volume', 'macd_line', 'macd_diff', 'macd_signal',\n",
      "       'rsi', 'vix_prct_returns', 'daily_prct_change', 'ebitda', 'ev_ebitda',\n",
      "       'daily_return', 'log_return', 'target', 'time_idx', 'symbol'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"symbol\", \"time_idx\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "743bb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"target\"\n",
    "group_ids = [\"symbol\"]\n",
    "time_idx = \"time_idx\"\n",
    "\n",
    "# âœ… Features known in the future\n",
    "known_reals = [\"time_idx\"]\n",
    "\n",
    "# âœ… Observed only in the past\n",
    "unknown_reals = [\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"macd_line\",\n",
    "    \"macd_diff\",\n",
    "    \"macd_signal\",\n",
    "    \"rsi\",\n",
    "    \"vix_prct_returns\",\n",
    "    \"daily_prct_change\",\n",
    "    \"daily_return\",\n",
    "    \"log_return\"\n",
    "]\n",
    "\n",
    "# âœ… Static (fundamentals)\n",
    "static_reals = [\n",
    "    \"ebitda\",\n",
    "    \"ev_ebitda\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "416af67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 60     # lookback window (2â€“3 months)\n",
    "max_prediction_length = 10 # forecast horizon (2 weeks)\n",
    "\n",
    "training_cutoff = df[time_idx].max() - max_prediction_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "607f416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TimeSeriesDataSet created successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    training = TimeSeriesDataSet(\n",
    "        df[df[time_idx] <= training_cutoff],\n",
    "        time_idx=time_idx,\n",
    "        target=target,\n",
    "        group_ids=group_ids,\n",
    "\n",
    "        static_reals=static_reals,\n",
    "        time_varying_known_reals=known_reals,\n",
    "        time_varying_unknown_reals=unknown_reals,\n",
    "\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "\n",
    "        target_normalizer=None,\n",
    "\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "\n",
    "        allow_missing_timesteps=True\n",
    "    )\n",
    "    print(\"âœ… TimeSeriesDataSet created successfully!\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(\"âŒ AssertionError:\", e)\n",
    "    print(\"Known Reals:\", known_reals)\n",
    "    print(\"Unknown Reals:\", unknown_reals)\n",
    "    print(\"Static Reals:\", static_reals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8776e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)\n",
    "\n",
    "train_loader = DataLoader(training, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(validation, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcc542ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    df,\n",
    "    predict=True,\n",
    "    stop_randomization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ff2cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    training,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "507752e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.001,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    output_size=7,    # quantiles\n",
    "    loss=QuantileLoss(),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d0fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "514847a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max time_idx: 3498\n",
      "Validation starts at time_idx > 3149\n"
     ]
    }
   ],
   "source": [
    "max_encoder_length = 60\n",
    "max_prediction_length = 1\n",
    "\n",
    "# Use last ~10% of time for validation\n",
    "max_time_idx = df[\"time_idx\"].max()\n",
    "val_cutoff = max_time_idx -  int(0.1 * (max_time_idx - df[\"time_idx\"].min()))\n",
    "\n",
    "print(\"Max time_idx:\", max_time_idx)\n",
    "print(\"Validation starts at time_idx >\", val_cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01ab6676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2172, 16)\n",
      "Val shape  : (238, 16)\n"
     ]
    }
   ],
   "source": [
    "# all features except target/time_idx/symbol/date\n",
    "feature_cols = [\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"macd_line\",\n",
    "    \"macd_diff\",\n",
    "    \"macd_signal\",\n",
    "    \"rsi\",\n",
    "    \"vix_prct_returns\",\n",
    "    \"daily_prct_change\",\n",
    "    \"ebitda\",\n",
    "    \"ev_ebitda\",\n",
    "    \"daily_return\",\n",
    "    \"log_return\",\n",
    "]\n",
    "\n",
    "training = df[df[\"time_idx\"] <= val_cutoff].copy()\n",
    "validation = df[df[\"time_idx\"] > val_cutoff].copy()\n",
    "\n",
    "print(\"Train shape:\", training.shape)\n",
    "print(\"Val shape  :\", validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c75ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TimeSeriesDataSet(\n",
    "    training,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=[\"symbol\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_encoder_length=max_encoder_length,     # fix encoder length\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=feature_cols + [\"target\"],\n",
    "    static_categoricals=[\"symbol\"],\n",
    "    target_normalizer=None,   # or leave None if target already scaled / log-return\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "validation_dataset = training_dataset.from_dataset(\n",
    "    training_dataset, validation, stop_randomization=True\n",
    ")\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = training_dataset.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation_dataset.to_dataloader(\n",
    "    train=False, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13a16bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline prediction shape: torch.Size([111, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:434: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "print(\"Baseline prediction shape:\", baseline_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f83cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 84.6k parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training_dataset,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    loss=QuantileLoss([0.1, 0.5, 0.9]),  # âœ… explicit quantiles\n",
    "    output_size=3,                      # âœ… MUST match number of quantiles\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Model has {tft.size()/1e3:.1f}k parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1d74d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pytorch_forecasting.models.temporal_fusion_transformer._tft.TemporalFusionTransformer'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.core import LightningModule\n",
    "\n",
    "print(type(tft))\n",
    "print(isinstance(tft, LightningModule))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47589914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode  | FLOPs\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train | 0    \n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train | 0    \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1      | train | 0    \n",
      "3  | prescalers                         | ModuleDict                      | 448    | train | 0    \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 96     | train | 0    \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 29.8 K | train | 0    \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train | 0    \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train | 0    \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train | 0    \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train | 0    \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train | 0    \n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train | 0    \n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train | 0    \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train | 0    \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train | 0    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train | 0    \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K  | train | 0    \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train | 0    \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train | 0    \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train | 0    \n",
      "20 | output_layer                       | Linear                          | 99     | train | 0    \n",
      "--------------------------------------------------------------------------------------------------------\n",
      "84.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.6 K    Total params\n",
      "0.339     Total estimated model params size (MB)\n",
      "361       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\john2\\anaconda3\\envs\\611\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:317: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:02<00:00,  7.83it/s, v_num=4, train_loss_step=0.0086, val_loss=0.013, train_loss_epoch=0.0107]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:02<00:00,  7.68it/s, v_num=4, train_loss_step=0.0086, val_loss=0.013, train_loss_epoch=0.0107]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=tft,   # âœ… MUST be named\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adc147b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pytorch_forecasting.models.temporal_fusion_transformer._tft.TemporalFusionTransformer'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Point prediction using median quantile (0.5)\n",
    "y_pred = tft.predict(val_dataloader).detach().cpu().numpy().flatten()\n",
    "\n",
    "# Ground truth\n",
    "y_true = x[\"decoder_target\"].detach().cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1df4b09e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lightning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlightning\u001b[49m.pytorch\n",
      "\u001b[31mNameError\u001b[39m: name 'lightning' is not defined"
     ]
    }
   ],
   "source": [
    "lightning.pytorch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "611",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
